# NLP_Frequency_distribution_and_Tokenization
Solved 7 problems using Word Tokenization and Frequency Distribution 

Those are:
1. Created own corpus consisting of 500 words.
2. Found the length of tokens.
3. Found the number of sentences in the corpus.
4. Used Frequency Distribution Function to find the occurrence of words.
5. Found the occurrence of particular word.
6. Displayed Top 5 highest frequency of words in the document.
7. Showed a dispersion plot for the top 5 re-occuring words.
